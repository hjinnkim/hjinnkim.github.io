---
title: "[Paper Review] Denoising Diffusion Probabilistic Models (NeurIPS 2020)"
categories:
  - Paper Review
tags:
  - Generative Models
  - Diffusion Models
  
excerpt: "[Paper Review] Denoising Diffusion Probabilistic Models (NeurIPS 2020)"

last_modified_at: 2025-01-18T22:00:00
---

# Denoising Diffusion Probabilistic Models (NeurIPS 2020)

> Denoising Diffusion Probabilistic Models. [[Paper](https://arxiv.org/abs/2006.11239)] [[Github](https://github.com/hojonathanho/diffusion)] \\
> Jonathan Ho, Ajay Jain, Pieter Abbeel \\
> Jun 19th 2020

Diffusion Model의 (거의) 시작점을 알린 논문이다. 이 논문을 처음 보았을 때 굉장히 어려웠었고, 그래서 이 논문을 가장 먼저 포스팅해보려고 한다. 혹시 관련 개념을 잘 모른다면, [[Intro to Diffusion Models] (1) What is Diffusion?](/study/what-is-diffusion/)를 보고 오시는 것을 추천한다.

또한, Diffusion Model의 수학을 굉장히 잘 설명해둔 [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970) 역시 큰 도움이 될 것이다. 이 포스팅 이후, 해당 논문을 리뷰할 예정이다.

## Background

Diffusion Model은 다음과 같은 형태를 띈다.

$$
x_0:=x(0)\sim p_{data}(x)\underset{p_\theta(x_0 \vert x_1)}{\overset{q(x_1 \vert x_0)}{\rightleftarrows}} x_1 \rightleftarrows \cdots \rightarrow x_{T-1} \underset{p_\theta(x_{T-1} \vert x_{T})}{\overset{q(x_{T} \vert x_{T-1})}{\rightleftarrows}} x_T:=x(1),\ x(1)\approx z\sim \mathcal N(0,I)
$$

이를 모델 $p_\theta$의 관점에서 표현하면, Reverse Diffusion Process를 묘사하는 다음의 latent variable model로 표현된다. 

$$
p_\theta(x_{0:T}):=p(x_T)\prod^T_{t=1}p_\theta(x_{t-1}\vert x_t),\quad p_\theta(x_{t-1}\vert \mu_\theta(x_t,t),\Sigma_\theta(x_t,t))
$$

이때, $p_\theta(x_{0:T})$는 posterior이자, Forward Diffusion Process인 

$$
q(x_{1:T}\vert x_0):=\prod^T_{t=1}q(x_t\vert x_{t-1}),\quad q(x_t\vert x_{t-1}):=\mathcal N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I)
$$

를 근사하도록 학습한다. 

<details>
<summary>Reverse Diffusion Process로 왜 Forward Diffusion Process를 근사하는가?</summary>
<div>

 만약 $p_\theta$가 $q$를 잘 근사한다면, $p_\theta(x_t)$가 묘사하는 분포와 $q(x_t)$가 묘사하는 분포가 동일해야한다. 따라서, Forward Diffusion Process를 묘사하는 $q$의 분포를 $p_\theta$를 통해 근사하는 것이다.

</div>
</details>
<br>

이때, $q(x_T\vert x_0)\approx \mathcal N(x_T;0,I)$여야 하기 때문에, $q(x_t\vert x_{0})$은 $t\rightarrow T$로 진행될 수록 $x_{t-1}$의 영향력을 줄여나가면서, variance를 $I$에 가깝게 만들어야 한다. 따라서 variance schedule, $\beta_1,\dots,\beta_T$는 $0<\beta_1,<\beta_2<\cdots<\beta_{T-1}<\beta_T<1$ 로 정의된다. 따라서, 이 과정을 거치면, $x_{t-1}$을 scaling down 하고 gaussian noise를 더하는 형태로 Diffusion Process를 진행하게 된다.

학습은 variational bound on negative log-likelihood를 최적화하여 $p_\theta$를 학습한다. 

$$
\begin{align}
\mathbb E\left[{-\log p_\theta(x_{0})}\right] &\leq \mathbb E_{q}\left[{ - \log \frac{p_\theta(x_{0:T})}{q(x_{1:T} | x_0)}}\right] \\ 
& = \mathbb{E}_q\left[ -\log p(x_T) - \sum_{t \geq 1} \log \frac{p_\theta(x_{t-1} | x_t)}{q(x_t|x_{t-1})} \right] := L
\end{align}
$$

이때, $\leq$는 [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality)에 의해 성립된다. $L$을 최적화하는 것은 $\log p_\theta(x_{0})$을 최대화하는 것과 같으므로, data $x_0$을 sampling할 확률을 높이는 방향으로 $p_\theta$를 학습한다는 뜻이다.
## Reparameterization trick

왜 gaussian noise를 더한다는 것인가? 이는 gaussian distribution에서의 sampling 과정을 살펴보면 명확해진다. (statistical machine learning에서 주로 쓰이며) 딥러닝에서는 [VAE 논문](https://arxiv.org/abs/1312.6114)에서 사용하여 유명해진 *Reparameterization trick*을 사용하여 gaussian distribution에서의 sampling을 진행한다. Back-propagation을 통해 gaussian distribution을 묘사하는 $\mu,\sigma^2$을 학습하기 위해서는, $\mu,\sigma$에 gradient가 흐르도록 $z\sim\mathcal N(\mu,\sigma^2)$을 sampling 해야한다. Reparameterization trick은 $z$를 다음과 같은 형태로 샘플링한다.

$$
z=\mu+\sigma\epsilon,\quad  \epsilon\sim \mathcal N(0,I)
$$

이 형태를 고려하여 $q(x_t\vert x_{t-1}):=\mathcal N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I)$를 살펴보면

$$
x_t=\sqrt{1-\beta_t}x_{t-1}+\beta_t I\epsilon_t,\quad \epsilon_t\sim \mathcal N(0,I)
$$

형태를 띈다. $0<\beta_t<1$ 이기 때문에, $x_t$에서 $x_{t-1}$은 scaling down되고,  $\beta_t$만큼 scaling된 gaussian noise $\epsilon_t$가 $\sqrt{1-\beta_t}x_{t-1}$에 더해진 형태로 $x_t$가 sampling 되는 것이다.

